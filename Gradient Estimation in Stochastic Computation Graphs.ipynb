{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Estimation in Stochastic Computation Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we aim at exploring different ways to **estimate gradients** in stochastic computation graphs in order to optimize an objective function defined by an expectation over a set of random variables.\n",
    "\n",
    "In terms of implementation, we'll use the TensorFlow library, mainly because of its automatic reverse-mode differentiation capabilities.\n",
    "\n",
    "The notation and theory are based on the following NIPS paper:\n",
    "<blockquote>\n",
    "Schulman, J., Heess, N., Weber, T. and Abbeel, P., 2015.<br>\n",
    "**[Gradient estimation using stochastic computation graphs](http://papers.nips.cc/paper/5899-gradient-estimation-using-stochastic-computation-graphs.pdf).**<br>\n",
    "In Advances in Neural Information Processing Systems (pp. 3528-3536).\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Computation Graphs\n",
    "\n",
    "A stochastic computation graph is a DAG (directly acyclic graph) defined over 3 different type of nodes:\n",
    "- *input nodes*: the parameters one wants to estimate the gradient with respect to;\n",
    "- *deterministic nodes*: representing deterministic dependencies on their parents (i.e., a function); and\n",
    "- *stochastic nodes*: representing random variables distributed conditionally on their parents.\n",
    "\n",
    "In this notebook, we will consider the following simple stochastic computation graph as a running example:\n",
    "![scg1](files/img/scg1.png)\n",
    "\n",
    "This stochastic computation graph encodes the expected loss:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}_{y~\\sim~p(\\cdot~|~x(\\theta))} [ f(y) ]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Our overall goal is to estimate the gradient $\\nabla_{\\theta} \\mathcal{L}$ in order to minimize the objective $\\mathcal{L}$ by stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the deterministic and stochastic dependencies in order to have a concrete example:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta &\\in [-\\infty, +\\infty] \\\\\n",
    "x(\\theta) &= \\text{sigmoid}(\\theta) = \\frac{1}{1+e^{-\\theta}} \\\\\n",
    "y &\\sim \\mathcal{N}(\\mu=x + 0.5, \\sigma=|2x - 1|) \\\\\n",
    "f(y) &= y(y - 2) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect how all these dependencies fit together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 3))\n",
    "\n",
    "# input parameter theta and deterministic function x(theta)\n",
    "theta = np.linspace(-10, 10, 1000)\n",
    "x = 1.0 / (1.0 + np.exp(-theta))\n",
    "plt.subplot(131)\n",
    "plt.plot(theta, x)\n",
    "plt.title('$x(\\\\theta) = sigmoid(\\\\theta)$')\n",
    "plt.xlabel('$\\\\theta$')\n",
    "plt.ylabel('x($\\\\theta$)')\n",
    "plt.grid()\n",
    "\n",
    "# random variable y distributed conditionally on x\n",
    "plt.subplot(132)\n",
    "for x in [1.0, 0.8, 0.6, 0.55]:\n",
    "    y = np.random.normal(loc=x + 0.5, scale=np.abs(2 * x - 1), size=(1000000))\n",
    "    plt.hist(y, bins=1000, histtype='step', normed=True, label='x = {}'.format(x))\n",
    "    plt.xlim(-1, 4)\n",
    "plt.title('$y\\sim\\mathcal{N}(\\mu=x + 0.5, \\sigma=|2x-1|)$')\n",
    "plt.xlabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# deterministic function f(y)\n",
    "y = np.linspace(-1, 3, 100)\n",
    "f = y * (y - 2)\n",
    "plt.subplot(133)\n",
    "plt.plot(y, f)\n",
    "plt.title('f(y) = y(y-2)')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('f(y)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this visual inspection, it should be clear that by decreasing the standard deviation of $y$ and at the same time getting the mean close to $1.0$, we'll be able to minimize the expected value of $f(y)$, since $argmin_y f(y) = 1.0$.\n",
    "\n",
    "So, to achieve this mininum value, it seems that we should simply set the input parameter $x$ to $0.5$, which correspond to set $\\theta$ to $0.0$.\n",
    "\n",
    "In summary, with this simple convex example we can build an analytical solution to the optimization problem, which will serve as a baseline to debug the code in next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the stochastic computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to build the computation graph in TensorFlow. Note that we've defined the input parameter as a placeholder so as to evaluate $\\mathcal{L}(\\theta)$ as a function of $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    theta = tf.placeholder(shape=(), dtype=tf.float32, name=\"theta\")\n",
    "    x = tf.sigmoid(theta, name=\"x\")\n",
    "\n",
    "    mu = tf.add(x, 0.5, name=\"mu\")\n",
    "    sigma = tf.abs(2*x - 1, name=\"sigma\")\n",
    "    y = tf.distributions.Normal(loc=mu, scale=sigma).sample(sample_shape=(batch_size), name=\"y\")\n",
    "\n",
    "    f = tf.multiply(y, y-2, name=\"f\")\n",
    "    loss = tf.reduce_mean(f, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate $\\mathcal{L}(\\theta)$ by running our graph in a session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    losses = []\n",
    "    theta_values = np.linspace(-10.0, 10.0, 1000)\n",
    "    for theta_value in theta_values:\n",
    "        expected_loss = sess.run(loss, feed_dict={theta: theta_value})\n",
    "        losses.append(expected_loss)\n",
    "\n",
    "# plot expected loss as a function of theta\n",
    "plt.plot(theta_values, losses)\n",
    "plt.title('Expected loss')\n",
    "plt.ylabel('$\\mathcal{L}(\\\\theta)$')\n",
    "plt.xlabel('$\\\\theta$')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen from the chart above, our initial hypothesis is correct: setting $\\theta = 0.0$ indeed minimizes the expected loss $\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the loss gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's set the hyper-parameters as globals so as to be used for all the remaining models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 100000\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent over the Stochastic Gradient Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first attempt, let's add the necessary operations in our model to run stochastic gradient desccent directly over the stochastic computation graph in order to optimize $\\mathcal{L}(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    theta = tf.Variable(initial_value=-1.0, dtype=tf.float32, name=\"theta\")\n",
    "    x = tf.sigmoid(theta, name=\"x\")\n",
    "\n",
    "    mu = tf.add(x, 0.5, name=\"mu\")\n",
    "    sigma = tf.abs(2*x - 1, name=\"sigma\")\n",
    "    y = tf.distributions.Normal(loc=mu, scale=sigma).sample(sample_shape=(batch_size), name=\"y\")\n",
    "\n",
    "    f = tf.multiply(y, y-2, name=\"f\")\n",
    "    loss = tf.reduce_mean(f, name=\"loss\")\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run gradient descent over a number of epochs to see if we can find the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    theta_values = []\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # evaluate graph\n",
    "        theta_, loss_ = sess.run([theta, loss])\n",
    "        theta_values.append(theta_)\n",
    "        losses.append(loss_)\n",
    "\n",
    "        # optimize parameter\n",
    "        sess.run(train_step)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('epoch = {}, theta = {}, loss = {}\\r'.format(epoch, theta_, loss_), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_results((theta_values, 'parameter', 'epoch', '$\\\\theta$'),\n",
    "                   (losses, 'losses', 'epoch', '$\\mathcal{L}$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-parameterization trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the re-parameterization trick to extract out the random variable in order to sample it out of our model.\n",
    "\n",
    "![scg2](files/img/scg2.png)\n",
    "\n",
    "This stochastic computation graph encodes the expected loss:\n",
    "$$\n",
    "\\mathbb{E}_{\\xi \\sim \\mathcal{N}(0.0, 1.0)} [f(y(x, \\xi))]\n",
    "$$\n",
    "where $y(x, \\xi) = \\mu(x) + \\sigma(x)\\xi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    theta = tf.Variable(initial_value=-1.0, dtype=tf.float32, name=\"theta\")\n",
    "    x = tf.sigmoid(theta, name=\"x\")\n",
    "\n",
    "    noise = tf.placeholder(tf.float32, shape=(batch_size), name=\"noise\")\n",
    "\n",
    "    mu = tf.add(x, 0.5, name=\"mu\")\n",
    "    sigma = tf.abs(2*x - 1, name=\"sigma\")\n",
    "    y = tf.add(mu, sigma * noise, name=\"y\")\n",
    "\n",
    "    f = tf.multiply(y, y-2, name=\"f\")\n",
    "    loss = tf.reduce_mean(f, name=\"loss\")\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    theta_values = []\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # evaluate graph\n",
    "        noise_ = np.random.normal(loc=0.0, scale=1.0, size=(batch_size))\n",
    "        theta_, loss_ = sess.run([theta, loss], feed_dict={noise: noise_})\n",
    "        theta_values.append(theta_)\n",
    "        losses.append(loss_)\n",
    "\n",
    "        # optimize parameter\n",
    "        sess.run(train_step, feed_dict={noise: noise_})\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('epoch = {}, theta = {}, loss = {}\\r'.format(epoch, theta_, loss_), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_results((theta_values, 'parameter', 'epoch', '$\\\\theta$'),\n",
    "                   (losses, 'losses', 'epoch', '$\\mathcal{L}$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood ratio estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood ratio is a trick that allows us to write:\n",
    "$$\n",
    "\\nabla_{\\theta} p(y;\\theta) = p(y;\\theta) \\nabla_{\\theta} \\log p(y;\\theta)\n",
    "$$\n",
    "assuming that $p(y;\\theta) > 0$ is a continuous function for all $y$.\n",
    "\n",
    "\n",
    "Let's apply the likelihood ratio trick to our objective function $\\mathcal{L}(\\theta) = \\mathbb{E}_{y} [ f(y) ]$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta} \\mathcal{L}(\\theta) &= \\nabla_{\\theta} \\int_{y} p(y|x(\\theta))~f(y)~dy & &\\text{# defn expectation}\\\\\n",
    "&= \\int_{y} \\nabla_{\\theta} p(y|x(\\theta))~f(y)~dy & &\\text{# Lebesgue Integral} \\\\\n",
    "&= \\int_{y} p(y|x(\\theta)) \\nabla_{\\theta} \\log p(y|x(\\theta))~f(y)~dy & &\\text{# likelihood ratio trick} \\\\\n",
    "&= \\int_{y} p(y|x(\\theta)) [ \\nabla_{\\theta} \\log p(y|x(\\theta))~f(y)]~dy & &\\text{# associativity} \\\\\n",
    "&= \\mathbb{E}_{y} [ f(y) \\nabla_{\\theta} \\log p(y|x(\\theta))] & &\\text{# defn expectation} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This way we can estimate the gradient by:\n",
    "$$\n",
    "\\nabla_{\\theta} \\mathcal{L}(\\theta) \\approx \\frac{1}{m} \\sum_{i=1}^{m} [ f(y_i) \\nabla_{\\theta} \\log p(y_i|x(\\theta))]\n",
    "$$\n",
    "\n",
    "It can be shown that this is equivalent to define a **surrogate loss function** that takes into consideration the likelihood ratio into the loss function:\n",
    "\n",
    "![scg3](files/img/scg3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    theta = tf.Variable(initial_value=-1.0, dtype=tf.float32, name=\"theta\")\n",
    "    x = tf.sigmoid(theta, name=\"x\")\n",
    "\n",
    "    mu = tf.add(x, 0.5, name=\"mu\")\n",
    "    sigma = tf.abs(2*x - 1, name=\"sigma\")\n",
    "    p_y = tf.distributions.Normal(loc=mu, scale=sigma, name='p_y')\n",
    "\n",
    "    y = p_y.sample(sample_shape=(batch_size), name=\"y\")\n",
    "    f = tf.multiply(y, y-2, name=\"f\")\n",
    "    loss = tf.reduce_mean(f, name=\"loss\")\n",
    "\n",
    "    log_prob = p_y.log_prob(y, name=\"log_prob\")\n",
    "    grad_log_prob, = tf.gradients(xs=theta, ys=(f * log_prob), stop_gradients=[f])\n",
    "    grad_theta = grad_log_prob / batch_size\n",
    "\n",
    "    # stochastic gradient descent\n",
    "    new_theta = theta.assign(theta - learning_rate * grad_theta)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    theta_values = []\n",
    "    losses = []\n",
    "    gradients = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # evaluate graph\n",
    "        theta_, loss_, grad_theta_ = sess.run([theta, loss, grad_theta])\n",
    "        theta_values.append(theta_)\n",
    "        losses.append(loss_)\n",
    "        gradients.append(grad_theta_)\n",
    "\n",
    "        # optimize parameter\n",
    "        sess.run(new_theta)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('epoch = {}, theta = {}, loss = {}\\r'.format(epoch, theta_, loss_), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_results((theta_values, 'parameter', 'epoch', '$\\\\theta$'),\n",
    "                   (losses, 'losses', 'epoch', '$\\mathcal{L}$'),\n",
    "                   (gradients, 'gradients', 'epoch', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that stochastic gradient descent is having problems because of the gradients spikes near $\\theta \\approx 0$. This is known as exploding gradient problem.\n",
    "\n",
    "Let's try to get around this nuisance by clipping the value of gradient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    theta = tf.Variable(initial_value=-1.0, dtype=tf.float32, name=\"theta\")\n",
    "    x = tf.sigmoid(theta, name=\"x\")\n",
    "\n",
    "    mu = tf.add(x, 0.5, name=\"mu\")\n",
    "    sigma = tf.abs(2*x - 1, name=\"sigma\")\n",
    "    p_y = tf.distributions.Normal(loc=mu, scale=sigma, name='p_y')\n",
    "\n",
    "    y = p_y.sample(sample_shape=(batch_size), name=\"y\")\n",
    "    f = tf.multiply(y, y-2, name=\"f\")\n",
    "    loss = tf.reduce_mean(f, name=\"loss\")\n",
    "\n",
    "    log_prob = p_y.log_prob(y, name=\"log_prob\")\n",
    "    grad_log_prob, = tf.gradients(xs=theta, ys=(f * log_prob), stop_gradients=[f])\n",
    "    grad_theta = grad_log_prob / batch_size\n",
    "    grad_theta = tf.clip_by_value(grad_theta, -1.0, 1.0, name=\"grad_theta\")  # limiting the exploding gradients\n",
    "\n",
    "    # stochastic gradient descent\n",
    "    new_theta = theta.assign(theta - learning_rate * grad_theta)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    theta_values = []\n",
    "    losses = []\n",
    "    gradients = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # evaluate graph\n",
    "        theta_, loss_, grad_theta_ = sess.run([theta, loss, grad_theta])\n",
    "        theta_values.append(theta_)\n",
    "        losses.append(loss_)\n",
    "        gradients.append(grad_theta_)\n",
    "\n",
    "        # optimize parameter\n",
    "        sess.run(new_theta)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('epoch = {}, theta = {}, loss = {}\\r'.format(epoch, theta_, loss_), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_results((theta_values, 'parameter', 'epoch', '$\\\\theta$'),\n",
    "                   (losses, 'losses', 'epoch', '$\\mathcal{L}$'),\n",
    "                   (gradients, 'gradients', 'epoch', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great! We no more have huge spikes in the gradients. Note how the gradients are limited to the maximum value given by the clipping operation.\n",
    "\n",
    "Additionally, it seems that we found the optimal value of $\\theta$ much a lot faster (before epoch 200 when other methods needed around 800 epoch for the same learning rate and batch size).\n",
    "\n",
    "It should be clear that this is a simple way to deal with exploding gradients. Of course, more robust optimization methods can be used.\n",
    "\n",
    "See you next time, bro!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
